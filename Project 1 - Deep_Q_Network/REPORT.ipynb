{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "colab": {
      "name": "REPORT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Cou2WTiDSV"
      },
      "source": [
        "## Project Navigation - Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh9ll1WAiDSo"
      },
      "source": [
        "### Training step\n",
        "\n",
        "We construct the **agent** with different parameters at each steps of training.\n",
        "Subsquently, we run the *Deep-Q-Network* procedure **dqn** as follows:\n",
        "\n",
        "        agent = Agent(state_size=37, action_size=4, seed=1, fc1_units=fc1_nodes, fc2_units=fc2_nodes)       \n",
        "        scores, episodes = **dqn**(n_episodes = 2000, eps_start = epsilon_start, train_numb=i)  \n",
        "    \n",
        "The file 'weights_'+str(train_numb)+'.trn' is  the obtained weights saved file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZdHSsydiDSr"
      },
      "source": [
        "### Deep-Q-Network algorithm\n",
        "\n",
        "The order of _Deep-Q-Network_ **dqn** excutes the external loop (by _episodes_) till the number of episodes \n",
        "hits the max num of episodes _n_episodes = 2000_.\n",
        "We check the below for looking up to the how many episodes are run \n",
        "\n",
        "        np.mean(scores_window) >= 13,  \n",
        "\n",
        "where _scores_\\__window_ is the array of the type deque realizing  the shifting window of length <= 100.\n",
        "The _score_ is contained in the element _scores_\\__window_[i] then it is achieved by the algorithm on the episode _i_.\n",
        "\n",
        "\n",
        "internaly,  **dqn** gets the current _action_ from the **agent**.\n",
        "By this _action_ **dqn** gets _state_ and _reward_ from Unity environment _env_.\n",
        "Then, the **agent** accept params _state_, _action_, _reward_, _next_\\__state_, _done_\n",
        "At the following training step. The _score_ accumulates attained rewards.\n"
      ]
    },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyper Parameter\n",
    "\n",
    "* BUFFER_SIZE : int(1e5) # replay buffer size"
    "* BATCH_SIZE : 64 # minibatch size"
    "* GAMMA: 0.99 # discount factor"
    "* TAU : 1e-3 # for soft update of target parameters"
    "* LR : 5e-4 # learning rate"
    "* UPDATE_EVERY : 4 # how often to update the network"
   ]
  },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVBpmlK4iDSt"
      },
      "source": [
        "### Mechanisms of Agent\n",
        "\n",
        "The class **Agent** is is the well-known class implementing the following mechanisms:\n",
        "\n",
        "* Two Q-Networks (local and target) using the simple neural network.\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
        "\n",
        "* Replay memory (using the class ReplayBuffer)\n",
        "\n",
        "       self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "       ...\n",
        "       e = self.experience(state, action, reward, next_state, done)\n",
        "       self.memory.append(e)\n",
        "     \n",
        "* Epsilon-greedy mechanism\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "            \n",
        "   The epsilon become a bit smaller with each episode:\n",
        "   \n",
        "        eps = max(eps_end, eps_decay*eps), \n",
        "        \n",
        "where eps_end=0.01, eps_decay = 0.996.        \n",
        "   \n",
        "* Q-learning, i.e., using the max value for all possible actions\n",
        "* Computing the loss function by MSE loss\n",
        "\n",
        "       loss = F.mse_loss(Q_expected, Q_targets)\n",
        "     \n",
        "* Minimize the loss by gradient descend mechanism using the ADAM optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upEvc69CiDSv"
      },
      "source": [
        "### Model Q-Network\n",
        "\n",
        "Both Q-Networks (local and target) are implemented by the class\n",
        "**QNetwork**. This class implements the simple neural network    \n",
        "with 3 fully-connected layers and 2 rectified nonlinear layers.\n",
        "The class **QNetwork** is realized in the framework of package **PyTorch**.   \n",
        "The number of neurons of the fully-connected layers are as follows:\n",
        "\n",
        " * Layer fc1,  number of neurons: _state_\\__size_ x _fc1_\\__units_, \n",
        " * Layer fc2,  number of neurons: _fc1_\\__units_ x _fc2_\\__units_,\n",
        " * Layer fc3,  number of neurons: _fc2_\\__units_ x _action_\\__size_,\n",
        " \n",
        "where _state_\\__size_ = 37, _action_\\__size_ = 8, _fc1_\\__units_ and _fc2_\\__units_\n",
        "are the input params."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot6JaAjZiDSy"
      },
      "source": [
        "### Training and Testing\n",
        "\n",
        "[image2]: layers_48x32_579ep.png  \"im3_48x32_579ep\"\n",
        "[image3]: layers_80x88_572ep.png  \"im4_80x88_572ep\"\n",
        "[image4]: layers_64x56_590ep.png  \"im5_64x56_590ep\"\n",
        "[image5]: layers_80x88_633ep.png  \"im6_80x88_633ep\"\n",
        " \n",
        "We run 5 training sessions with different parameters _fc1_\\__units_,  _fc2_\\__units_, _eps_\\__start_,\n",
        "and we save obtained weights by the function of **PyTorch**:\n",
        "\n",
        "    torch.save(agent.qnetwork_local.state_dict(), 'weights_'+str(train_numb)+'.trn') \n",
        "     \n",
        "<br>This is the typical output of training sessions:\n",
        "\n",
        "<br>fc1_units:  64 , fc2_units:  64\n",
        "train_numb:  0 eps_start:  0.975\n",
        "Episode: 538, elapsed: 0:10:51.311591, Avg.Score: 13.02,  score 19.0, How many scores >= 13: 59, eps.: 0.11\n",
        "<br>terminating at episode : 538 ave reward reached +13 over 100 episodes    \n",
        " ![im2_96x88_585ep][image2]      \n",
        " \n",
        "<br>fc1_units:  64 , fc2_units:  72\n",
        "train_numb:  1 eps_start:  0.974\n",
        "Episode: 594, elapsed: 0:11:57.215037, Avg.Score: 13.01,  score 14.0, How many scores >= 13: 62, eps.: 0.09\n",
        "<br>erminating at episode : 594 ave reward reached +13 over 100 episodes\n",
        " ![im3_48x32_579ep][image3]   \n",
        " \n",
        "<br>fc1_units:  48 , fc2_units:  56\n",
        "train_numb:  2 eps_start:  0.979\n",
        "Episode: 578, elapsed: 0:11:36.222573, Avg.Score: 13.04,  score 13.0, How many scores >= 13: 56, eps.: 0.10\n",
        "<br>terminating at episode : 578 ave reward reached +13 over 100 episodes\n",
        " ![im4_80x88_572ep][image4]   \n",
        "\n",
        "<br>fc1_units:  64 , fc2_units:  56\n",
        "train_numb:  3 eps_start:  0.995\n",
        "Episode: 594, elapsed: 0:11:51.450521, Avg.Score: 13.06,  score 17.0, How many scores >= 13: 59, eps.: 0.09\n",
        "<br>terminating at episode : 594 ave reward reached +13 over 100 episodes   \n",
        " ![im5_64x56_590ep][image5]   \n",
        "\n",
        "<br>fc1_units:  112 , fc2_units:  96\n",
        "train_numb:  4 eps_start:  0.975\n",
        "Episode: 29, elapsed: 0:00:33.691869, Avg.Score: 0.24,  score 0.0, How many scores >= 13: 0, eps.: 0.878  \n",
        "\n",
        "\n",
        "<br>After that we go to the testing session. For each session, we load saved weights by the function of **PyTorch** \n",
        "as follows:\n",
        "\n",
        "    file_weights = 'weights_'+str(train_n)+'.trn'\n",
        "    agent.qnetwork_local.load_state_dict(torch.load(file_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sQ95V-EiDS5"
      },
      "source": [
        "### Solved environment (from my local machine)\n",
        "\n",
        "We get the average score > 13 in 5 from 6 testing sessions.  The number of episodes needed to reach this score \n",
        "less than 600. Only in the case fc1_units = 96, fc2_units = 96 we get the average score < 13, however in this case\n",
        "in 3 tests from 6 we get the average score > 13.  From the examples that I have seen so far (on my local machine and on\n",
        "the workspace machine), we get better results when fc1_inits > fc2_units. "
      ]
    },
    {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Future work \n",
    "\n",
    "The future works for improving the agent's performance.\n",
    "\n",
    "1. Improve the performance of the overal network by adding several nonlinear layers to the neural network (linear layers also will be tried to compare).\n",
    "\n",
    "2. Try different numbers of neurons per NN layer to get the deepr knowledge on the behavior of DL networks.\n",
    "   It is interesting to catch a good relation between fc1_units and fc2_inits.\n",
    "\n",
    "3. Find more details on epsilon-greedy policy by trying different staring value of epsilon and check if it can improve the network.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### References to possible improvements\n",
    "\n",
    "It would be very useful to check improvements in [Deep Q Learning: Dueling Double DQN, Prioritized Experience Replay, and fixed Q-targets](https://medium.freecodecamp.org/improvements-in-deep-q-learning-dueling-double-dqn-prioritized-experience-replay-and-fixed-58b130cc5682)\n",
    "\n",
    "One effective way to improve the performance is by using Prioritized Experience Replay. It will be interesting to check [github repo](https://github.com/rlcode/per) for a fast implementation of Prioritized Experience Replay using a special data structure Sum Tree.\n",
    "\n",
    "### The recent achievement \n",
    "\n",
    "[Open AI group to play Dota 2](https://openai.com/blog/dota-2/) using Reinforcement Learning. They have created a bot which beats the worldâ€™s top professionals at 1v1 matches of [Dota 2](http://blog.dota2.com/?l=english) under standard tournament rules. The bot learned the game from scratch by self-play, and does not use imitation learning or tree search. This is a step towards building AI systems which accomplish well-defined goals in messy, complicated situations involving real humans. \n",
    "\n",
    "### Competitive Self-Play\n",
    "\n",
    "[Push opponent outside the ring and other](https://www.youtube.com/watch?v=OBcjhp4KSgQ)\n",
    "\n",
    "### Blog\n",
    "\n",
    "[Tackling, ducking, faking, kicking, catching, and diving for the ball](https://openai.com/blog/competitive-self-play/)\n",
    "\n",
    "### Tensorflow or PyTorch\n",
    "Tensorflow is based on Theano and has been developed by Google, whereas PyTorch is based on Torch and has been developed by Facebook. [The force is strong with which one?](https://medium.com/@UdacityINDIA/tensorflow-or-pytorch-the-force-is-strong-with-which-one-68226bb7dab4)\n",
    "\n"
   ]
  },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzHiw2X6sQed"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPtaN0yBiDS_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
