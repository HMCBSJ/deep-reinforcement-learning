{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "kernelspec": {
      "display_name": "Python 2",
      "language": "python",
      "name": "python2"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.14"
    },
    "colab": {
      "name": "REPORT.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "y5Cou2WTiDSV"
      },
      "source": [
        "## Project Navigation - Report"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hh9ll1WAiDSo"
      },
      "source": [
        "### Training step\n",
        "\n",
        "We construct the **agent** with different parameters at each steps of training.\n",
        "Subsquently, we run the *Deep-Q-Network* procedure **dqn** as follows:\n",
        "\n",
        "        agent = Agent(state_size=37, action_size=4, seed=1, fc1_units=fc1_nodes, fc2_units=fc2_nodes)       \n",
        "        scores, episodes = **dqn**(n_episodes = 2000, eps_start = epsilon_start, train_numb=i)  \n",
        "    \n",
        "The file 'weights_'+str(train_numb)+'.trn' is  the obtained weights saved file.\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kZdHSsydiDSr"
      },
      "source": [
        "### Deep-Q-Network algorithm\n",
        "\n",
        "The order of _Deep-Q-Network_ **dqn** excutes the external loop (by _episodes_) till the number of episodes \n",
        "hits the max num of episodes _n_episodes = 2000_.\n",
        "We check the below for looking up to the how many episodes are run \n",
        "\n",
        "        np.mean(scores_window) >= 13,  \n",
        "\n",
        "where _scores_\\__window_ is the array of the type deque realizing  the shifting window of length <= 100.\n",
        "The _score_ is contained in the element _scores_\\__window_[i] then it is achieved by the algorithm on the episode _i_.\n",
        "\n",
        "\n",
        "internaly,  **dqn** gets the current _action_ from the **agent**.\n",
        "By this _action_ **dqn** gets _state_ and _reward_ from Unity environment _env_.\n",
        "Then, the **agent** accept params _state_, _action_, _reward_, _next_\\__state_, _done_\n",
        "At the following training step. The _score_ accumulates attained rewards.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZVBpmlK4iDSt"
      },
      "source": [
        "### Mechanisms of Agent\n",
        "\n",
        "The class **Agent** is is the well-known class implementing the following mechanisms:\n",
        "\n",
        "* Two Q-Networks (local and target) using the simple neural network.\n",
        "\n",
        "        self.qnetwork_local = QNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
        "        self.qnetwork_target = QNetwork(state_size, action_size, seed, fc1_units, fc2_units).to(device)\n",
        "\n",
        "* Replay memory (using the class ReplayBuffer)\n",
        "\n",
        "       self.experience = namedtuple(\"Experience\", field_names=[\"state\", \"action\", \"reward\", \"next_state\", \"done\"])\n",
        "       ...\n",
        "       e = self.experience(state, action, reward, next_state, done)\n",
        "       self.memory.append(e)\n",
        "     \n",
        "* Epsilon-greedy mechanism\n",
        "\n",
        "        if random.random() > eps:\n",
        "            return np.argmax(action_values.cpu().data.numpy())\n",
        "        else:\n",
        "            return random.choice(np.arange(self.action_size))\n",
        "            \n",
        "   The epsilon become a bit smaller with each episode:\n",
        "   \n",
        "        eps = max(eps_end, eps_decay*eps), \n",
        "        \n",
        "where eps_end=0.01, eps_decay = 0.996.        \n",
        "   \n",
        "* Q-learning, i.e., using the max value for all possible actions\n",
        "* Computing the loss function by MSE loss\n",
        "\n",
        "       loss = F.mse_loss(Q_expected, Q_targets)\n",
        "     \n",
        "* Minimize the loss by gradient descend mechanism using the ADAM optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "upEvc69CiDSv"
      },
      "source": [
        "### Model Q-Network\n",
        "\n",
        "Both Q-Networks (local and target) are implemented by the class\n",
        "**QNetwork**. This class implements the simple neural network    \n",
        "with 3 fully-connected layers and 2 rectified nonlinear layers.\n",
        "The class **QNetwork** is realized in the framework of package **PyTorch**.   \n",
        "The number of neurons of the fully-connected layers are as follows:\n",
        "\n",
        " * Layer fc1,  number of neurons: _state_\\__size_ x _fc1_\\__units_, \n",
        " * Layer fc2,  number of neurons: _fc1_\\__units_ x _fc2_\\__units_,\n",
        " * Layer fc3,  number of neurons: _fc2_\\__units_ x _action_\\__size_,\n",
        " \n",
        "where _state_\\__size_ = 37, _action_\\__size_ = 8, _fc1_\\__units_ and _fc2_\\__units_\n",
        "are the input params."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ot6JaAjZiDSy"
      },
      "source": [
        "### Training and Testing \n",
        " \n",
        "We run 5 training sessions with different parameters _fc1_\\__units_,  _fc2_\\__units_, _eps_\\__start_,\n",
        "and we save obtained weights by the function of **PyTorch**:\n",
        "\n",
        "    torch.save(agent.qnetwork_local.state_dict(), 'weights_'+str(train_numb)+'.trn') \n",
        "     \n",
        "For input: fc1_units = 45, fc2_units = 41, we get the following training output:   \n",
        "\n",
        "train_numb:  0 eps_start:  0.975\n",
        "Episode: 538, elapsed: 0:10:51.311591, Avg.Score: 13.02,  score 19.0, How many scores >= 13: 59, eps.: 0.11\n",
        " terminating at episode : 538 ave reward reached +13 over 100 episodes \n",
        "\n",
        "![](layers_48x40.png)\n",
        "\n",
        "\n",
        "After that we go to the testing session. For each session, we load saved weights by the function of **PyTorch** \n",
        "as follows:\n",
        "\n",
        "    file_weights = 'weights_'+str(train_n)+'.trn'\n",
        "    agent.qnetwork_local.load_state_dict(torch.load(file_weights))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7sQ95V-EiDS5"
      },
      "source": [
        "### Solved environment (from my local machine)\n",
        "\n",
        "We get the average score > 13 in 5 from 6 testing sessions.  The number of episodes needed to reach this score \n",
        "less than 600. Only in the case fc1_units = 96, fc2_units = 96 we get the average score < 13, however in this case\n",
        "in 3 tests from 6 we get the average score > 13.  From the examples that I have seen so far (on my local machine and on\n",
        "the workspace machine), we get better results when fc1_inits > fc2_units. "
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hzHiw2X6sQed"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DPtaN0yBiDS_"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}