{"nbformat":4,"nbformat_minor":0,"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.8"},"colab":{"name":"REPORT.ipynb","provenance":[]}},"cells":[{"cell_type":"markdown","metadata":{"id":"9bhwy4Fmkfui"},"source":["## Report - Project Collaboration and Competition"]},{"cell_type":"markdown","metadata":{"id":"1VtP8c_Gkfuy"},"source":["### Methods : DDPG and MADDPG Algorithms "]},{"cell_type":"markdown","metadata":{"id":"PLkwKGickfu0"},"source":["In this collaboration and competition project, we deploy the **DDPG** algorithm along with the **MADDPG** algorithm, which is a **Multi-Agent DDPG** as DDPG wrapper. When it comes to DDPG, it can simultaneously learn a Q-function and it's policy. Next, it uses off-policy data and the Bellman equation to learn the Q-function. Fianlly, DDP learns the policy by using the Q-function. This paired mechanism is called a actor-critic method.  \n","\n","\n","> Two additional mechanisms: _Replay Buffer_ and _Soft Updates_.\n","\n","\n","\n","For MADDPG algorithm, we train two separate agents to be the competitors to eachother, specifically, we let them **collaborate** and **compete**. MADDPG method comes in handy to get better result, compared to the original DDPG, in that, the original DDPG repeat a simple extension of single agent RL by independently training the two agents. It hardly works very well since the agents are individually updating their policiess when learning the prediction . Furthermore, this causes the environment to look like a non-stationary process from the viewpoint among one of the agent. \n","\n","When it comes to MADDPG, the critics of the _each agent are trained by the observations and actions_ that comes from **both agents** , while the actors of each _agent are trained by just_ which is their **own observations**.  \n","\n","In the finction _step()_ of the _class madppg_\\__agent_, we gether all of the current info\n"," for **both agents**  into  the **common** variable    \n","_memory_ of the type  _ReplayBuffer_.  After that, we attain the random _sample_ and then move it from _memory_  into the variable _experiance_.   \n","Then, this _experiance_ altogether with the current number of agent (0 or 1) moves to the function _learn()_. Then, we finally get the corresponding    \n","agent (of type _ddpg_\\__agent_):\n","\n","      agent = self.agents[agent_number]\n","\n","and then _experiance_ is transferred to function _learn()_  of the _class ddpg_\\__agent_.  At the point, the actor and the critic are dealt with different ways.  \n","\n","\n","\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"_5sHSJQ5kfu3"},"source":[" ### Eight Neural Networks"]},{"cell_type":"markdown","metadata":{"id":"U-BXWRh3kfu4"},"source":["In this prohect, there are 8 _neural networks_.  For the training, we \n","create one _maddpg agent_. \n","\n","         maddpg = maddpg_agent()\n","         \n","In turn, _maddpg agent_ creates 2 _ddpg agents_: \n","         \n","         self.agents = [ddpg_agent(state_size, action_size, i+1, random_seed=0) \n","                  for i in range(num_agents)]    \n","\n","Each of two agents (red and blue) create 4 neural networks:\n","\n","        self.actor_local = Actor(state_size, action_size).to(device)\n","        self.actor_target = Actor(state_size, action_size).to(device)\n","\n","        self.critic_local = Critic(state_size, action_size).to(device)\n","        self.critic_target = Critic(state_size, action_size).to(device)\n","\n","Classes Actor and Critic are provided by **model.py**. The typical behavior of the actor \n","\n","        actor_target(state) -> next_actions\n","        actor_local(states) -> actions_pred\n","        \n","see function _learn()_ in maddpg agent. The typical behavior of the critic is as follows:\n","\n","        critic_target(state, action) -> Q-value \n","        -critic_local(states, actions_pred) -> actor_loss\n","        \n","see function _learn()_ in ddpg agent.        \n","        "]},{"cell_type":"markdown","metadata":{"id":"gk7ILB42kfu6"},"source":["### Architecture of the actor and critic networks"]},{"cell_type":"markdown","metadata":{"id":"TyKSP8BMkfu7"},"source":["Both the actor and critic classes implement the neural network\n","with 3 fully-connected layers and 2 rectified nonlinear layers. These networks are realized in the framework\n","of package PyTorch. Such a network is used in Udacity model.py code for the Pendulum model using DDPG.\n","The number of neurons of the fully-connected layers are as follows:\n","\n","for the actor:   \n","Layer fc1, number of neurons: state_size x fc1_units,   \n","Layer fc2, number of neurons: fc1_units x fc2_units,   \n","Layer fc3, number of neurons: fc2_units x action_size,   \n","\n","for the critic:   \n","Layer fcs1, number of neurons: (state_size + action_size) x n_agents x fcs1_units,   \n","Layer fc2, number of neurons: (fcs1_units x fc2_units,   \n","Layer fc3, number of neurons: fc2_units x 1.   \n","\n","Here, state_size = 24, action_size = 2.       \n","The input parameters fc1_units, fc2_units, fcs1_units are all taken = 64.   "]},{"cell_type":"markdown","metadata":{"id":"HLzFMtuLkfu_"},"source":["### Hyperparameters"]},{"cell_type":"markdown","metadata":{"id":"nQPs3kuBkfvA"},"source":["From **ddpg_agent.py** \n","\n","        GAMMA = 0.99                    # discount factor  \n","        TAU = 5e-2                      # for soft update of target parameters   \n","        LR_ACTOR = 5e-4                 # learning rate of the actor   \n","        LR_CRITIC = 5e-4                # learning rate of the critic  \n","        WEIGHT_DECAY = 0.0              # L2 weight decay   \n","        NOISE_AMPLIFICATION = 1         # exploration noise amplification  \n","        NOISE_AMPLIFICATION_DECAY = 1   # noise amplification decay\n","\n","From **maddpg_agent.py**\n","\n","        BUFFER_SIZE = int(1e6)          # replay buffer size   \n","        BATCH_SIZE = 512                # minibatch size   \n","        LEARNING_PERIOD = 2             # weight update frequency \n","        \n","Note that parameters LEARNING_PERIOD is important. The corresponding code is in the function   _step()_.\n","\n","     if len(self.memory) > BATCH_SIZE and timestep % LEARNING_PERIOD == 0: \n","         for a_i, agent in enumerate(self.agents):\n","              experiences = self.memory.sample()\n","              self.learn(experiences, a_i)\n","\n"]},{"cell_type":"markdown","metadata":{"id":"PxRKWFA4kfvC"},"source":["### Training the Agent"]},{"cell_type":"markdown","metadata":{"id":"hC5K11-pkfvD"},"source":["On my local machine with GPU, the desired average reward **+0.5** was achieved in **1302** episodes in **18** minutes.\n","However, the training continued until **1700** episodes, where average reward was achived **+1.42** in **1 hour 18 minutes**."]},{"cell_type":"markdown","metadata":{"id":"PSjY0Cy2kfvE"},"source":["### Full log"]},{"cell_type":"markdown","metadata":{"id":"W7PbPxDDkfvE"},"source":["\n","Episode: 20, Score: 0.0450, \tAverage Score: 0.0350, Time: 00:00:11<br>\n","Episode: 40, Score: 0.0450, \tAverage Score: 0.0463, Time: 00:00:26 <br>\n","Episode: 60, Score: 0.0450, \tAverage Score: 0.0433, Time: 00:00:39<br> \n","Episode: 80, Score: 0.0450, \tAverage Score: 0.0425, Time: 00:00:52 <br>\n","Episode: 100, Score: 0.0450, \tAverage Score: 0.0420, Time: 00:01:04 <br>\n","Episode: 120, Score: 0.0450, \tAverage Score: 0.0425, Time: 00:01:16 <br>\n","Episode: 140, Score: 0.0450, \tAverage Score: 0.0395, Time: 00:01:29 <br>\n","Episode: 160, Score: 0.0450, \tAverage Score: 0.0390, Time: 00:01:40 <br>\n","Episode: 180, Score: -0.0050, \tAverage Score: 0.0370, Time: 00:01:52 <br>\n","Episode: 200, Score: -0.0050, \tAverage Score: 0.0350, Time: 00:02:03 <br>\n","Episode: 220, Score: -0.0050, \tAverage Score: 0.0345, Time: 00:02:15 <br>\n","Episode: 240, Score: 0.0450, \tAverage Score: 0.0355, Time: 00:02:29 <br>\n","Episode: 260, Score: 0.0450, \tAverage Score: 0.0365, Time: 00:02:41 <br>\n","Episode: 280, Score: 0.0450, \tAverage Score: 0.0345, Time: 00:02:51 <br>\n","Episode: 300, Score: 0.0450, \tAverage Score: 0.0315, Time: 00:03:00 <br>\n","Episode: 320, Score: 0.1450, \tAverage Score: 0.0315, Time: 00:03:12 <br>\n","Episode: 340, Score: 0.0450, \tAverage Score: 0.0300, Time: 00:03:25 <br>\n","Episode: 360, Score: -0.0050, \tAverage Score: 0.0315, Time: 00:03:38 <br>\n","Episode: 380, Score: 0.0450, \tAverage Score: 0.0345, Time: 00:03:50 <br>\n","Episode: 400, Score: 0.0450, \tAverage Score: 0.0390, Time: 00:04:02 <br>\n","Episode: 420, Score: -0.0050, \tAverage Score: 0.0430, Time: 00:04:16 <br>\n","Episode: 440, Score: 0.0450, \tAverage Score: 0.0425, Time: 00:04:28 <br>\n","Episode: 460, Score: 0.0450, \tAverage Score: 0.0405, Time: 00:04:41 <br>\n","Episode: 480, Score: 0.0450, \tAverage Score: 0.0430, Time: 00:04:54 <br>\n","Episode: 500, Score: 0.0450, \tAverage Score: 0.0450, Time: 00:05:09 <br>\n","Episode: 520, Score: 0.0450, \tAverage Score: 0.0460, Time: 00:05:24 <br>\n","Episode: 540, Score: 0.0450, \tAverage Score: 0.0525, Time: 00:05:41 <br>\n","Episode: 560, Score: 0.1450, \tAverage Score: 0.0610, Time: 00:06:00 <br>\n","Episode: 580, Score: 0.0450, \tAverage Score: 0.0630, Time: 00:06:15 <br>\n","Episode: 600, Score: 0.0450, \tAverage Score: 0.0655, Time: 00:06:31 <br>\n","Episode: 620, Score: 0.0450, \tAverage Score: 0.0645, Time: 00:06:45 <br>\n","Episode: 640, Score: 0.0450, \tAverage Score: 0.0630, Time: 00:07:02 <br>\n","Episode: 660, Score: 0.0450, \tAverage Score: 0.0580, Time: 00:07:16 <br>\n","Episode: 680, Score: -0.0050, \tAverage Score: 0.0565, Time: 00:07:30 <br>\n","Episode: 700, Score: 0.0450, \tAverage Score: 0.0555, Time: 00:07:44 <br>\n","Episode: 720, Score: 0.0950, \tAverage Score: 0.0565, Time: 00:08:00 <br>\n","Episode: 740, Score: 0.0450, \tAverage Score: 0.0555, Time: 00:08:16 <br>\n","Episode: 760, Score: 0.0450, \tAverage Score: 0.0585, Time: 00:08:33 <br>\n","Episode: 780, Score: 0.0950, \tAverage Score: 0.0630, Time: 00:08:51 <br>\n","Episode: 800, Score: 0.0950, \tAverage Score: 0.0675, Time: 00:09:09 <br>\n","Episode: 820, Score: 0.0450, \tAverage Score: 0.0645, Time: 00:09:23 <br>\n","Episode: 840, Score: -0.0050, \tAverage Score: 0.0630, Time: 00:09:38 <br>\n","Episode: 860, Score: 0.1450, \tAverage Score: 0.0665, Time: 00:09:59 <br>\n","Episode: 880, Score: 0.0450, \tAverage Score: 0.0700, Time: 00:10:20 <br>\n","Episode: 900, Score: -0.0050, \tAverage Score: 0.0735, Time: 00:10:42 <br>\n","Episode: 920, Score: 0.1450, \tAverage Score: 0.0810, Time: 00:11:01 <br>\n","Episode: 940, Score: 0.0450, \tAverage Score: 0.0860, Time: 00:11:20 <br>\n","Episode: 960, Score: 0.0450, \tAverage Score: 0.0810, Time: 00:11:37 <br>\n","Episode: 980, Score: -0.0050, \tAverage Score: 0.0765, Time: 00:11:54 <br>\n","Episode: 1000, Score: 0.0950, \tAverage Score: 0.0760, Time: 00:12:17 <br>\n","Episode: 1020, Score: -0.0050, \tAverage Score: 0.0740, Time: 00:12:36 <br>\n","Episode: 1040, Score: 0.0450, \tAverage Score: 0.0755, Time: 00:12:56 <br>\n","Episode: 1060, Score: -0.0050, \tAverage Score: 0.0765, Time: 00:13:13 <br>\n","Episode: 1080, Score: -0.0050, \tAverage Score: 0.0785, Time: 00:13:33 <br>\n","Episode: 1100, Score: -0.0050, \tAverage Score: 0.0770, Time: 00:13:53 <br>\n","Episode: 1120, Score: 0.0450, \tAverage Score: 0.0705, Time: 00:14:08 <br>\n","Episode: 1140, Score: 0.0950, \tAverage Score: 0.0685, Time: 00:14:27 <br>\n","Episode: 1160, Score: -0.0050, \tAverage Score: 0.0690, Time: 00:14:45 <br>\n","Episode: 1180, Score: 0.1450, \tAverage Score: 0.0625, Time: 00:15:01 <br>\n","Episode: 1200, Score: 0.1450, \tAverage Score: 0.0610, Time: 00:15:20 <br>\n","Episode: 1220, Score: 0.2950, \tAverage Score: 0.0705, Time: 00:15:41 <br>\n","Episode: 1240, Score: 0.0450, \tAverage Score: 0.0700, Time: 00:15:59 <br>\n","Episode: 1260, Score: 0.0450, \tAverage Score: 0.0700, Time: 00:16:17 <br>\n","Episode: 1280, Score: 0.2450, \tAverage Score: 0.0780, Time: 00:16:38 <br>\n","Episode: 1300, Score: 0.1450, \tAverage Score: 0.0790, Time: 00:16:58 <br>\n","Episode: 1320, Score: 0.0450, \tAverage Score: 0.0715, Time: 00:17:14 <br>\n","Episode: 1340, Score: 0.0450, \tAverage Score: 0.0745, Time: 00:17:35 <br>\n","Episode: 1360, Score: 0.0450, \tAverage Score: 0.0700, Time: 00:17:49 <br>\n","Episode: 1380, Score: 0.1450, \tAverage Score: 0.0730, Time: 00:18:12 <br>\n","Episode: 1400, Score: 0.8950, \tAverage Score: 0.1025, Time: 00:18:57 <br>\n","Episode: 1420, Score: 0.0450, \tAverage Score: 0.1270, Time: 00:19:33 <br>\n","Episode: 1440, Score: 0.0950, \tAverage Score: 0.1771, Time: 00:20:35 <br>\n","Episode: 1460, Score: 0.0450, \tAverage Score: 0.1946, Time: 00:21:04 <br>\n","Episode: 1480, Score: 0.1950, \tAverage Score: 0.1876, Time: 00:21:22 <br>\n","Episode: 1500, Score: 0.7950, \tAverage Score: 0.1961, Time: 00:22:14 <br>\n","Episode: 1520, Score: 0.4950, \tAverage Score: 0.2186, Time: 00:23:09 <br>\n","Episode: 1540, Score: 0.0450, \tAverage Score: 0.1960, Time: 00:23:53 <br>\n","Episode: 1560, Score: 0.0450, \tAverage Score: 0.2000, Time: 00:24:26 <br>\n","Episode: 1580, Score: 0.2450, \tAverage Score: 0.2560, Time: 00:25:30 <br>\n","Episode: 1600, Score: 0.0450, \tAverage Score: 0.2285, Time: 00:25:59 <br>\n","Episode: 1620, Score: 1.1950, \tAverage Score: 0.2200, Time: 00:26:45 <br>\n","Episode: 1640, Score: 0.0450, \tAverage Score: 0.2260, Time: 00:27:35 <br>\n","Episode: 1660, Score: 0.0450, \tAverage Score: 0.2355, Time: 00:28:17 <br>\n","Episode: 1680, Score: 0.1450, \tAverage Score: 0.2245, Time: 00:29:15 <br>\n","Episode: 1700, Score: 0.0450, \tAverage Score: 0.2215, Time: 00:29:42  <br>"]},{"cell_type":"markdown","metadata":{"id":"SvG3WpgokfvM"},"source":["### Future ideas"]},{"cell_type":"markdown","metadata":{"id":"11Bc3AP2kfvT"},"source":["1. Check different values for hyperparameters such as LEARNING_PERIOD, and neural network parameters fc1_units, fc2_units, etc.\n","2. How does the addition of new nonlinear layers in the used neural networks affect the robustness of the algorithm.\n","3. It would be interesting to train agents using [MAPPO](https://github.com/kotogasy/unity-ml-tennis) and compare them with MADDPG. "]},{"cell_type":"code","metadata":{"id":"90fb-LuqkfvU"},"source":[""],"execution_count":null,"outputs":[]}]}